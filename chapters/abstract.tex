
Deep neural networks (DNNs) have exhibited state-of-the-art performance in various computer vision tasks; nevertheless, their substantial computational and memory requirements render them ill-suited for resource-constrained environments, such as space missions. In this context, stringent power consumption, processing throughput, and storage capacity constraints are further exacerbated by high levels of ionizing radiation. This radiation has the potential to induce bit flipping in model weights or activation values, which can result in critical failures. The present study investigates the relationship between model parameter reduction (measured by the number of floating-point operations saved) and accuracy degradation under systematic error injection. The objective of this investigation is to explore the fault tolerance of different complexity pruning models. Specifically, structured pruning is applied to large and small convolutional architectures (e.g., VGG and CCDF), with the injection of unit errors being controlled based on the number of floating-point operations in the model. The experimental findings demonstrate that substantial convolutional models, such as VGG-16, exhibit resilience to structured pruning, with a maximum tolerance of 50\% pruning, exhibiting minimal accuracy decline when subjected to radiation-induced faults. Conversely, compact models, such as CCDF\_MNIST, demonstrate a substantial decline in accuracy, exceeding 20 percentage points, when subjected to pruning greater than 30\%. These findings substantiate the evident trade-off between model compression and fault tolerance, thereby providing actionable guidance for the design of aerospace AI systems.
% Please keep '\par' and '\textbf' only edit 'Add around 5 keywords.'
\par\ \par
\textbf{Keywords}: Deep Neural Networks, Model Pruning, Model Quantization, Fault Injection, Model Robustness
