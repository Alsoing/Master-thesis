\chapter{Discussion and Related Work}
\label{chap:discussion}
In this chapter, we will explain and contextualize the results presented in chapters \ref{chap:imp} and \ref{chap:evaluation} as they relate to the broader literature on pruning, quantization, and fault tolerance. We will discuss the relationship between robustness and model compression, which models are best suited for environments characterized by radiation, the limitations of our current approach, and possible pathways for future work.

\section{Compression–Performance Trade-off}

Our iterative pruning method based on the L1-norm allows large reductions in computation while retaining a strong baseline. For example, using VGG-16 (CIFAR-10), we reduce \gls{flops} from 229.05M to 43.38M (80\% reduction) and the top-1 accuracy drops from 91.7\% to 88.0\% (–3.7\%). And using the compact CCDF\_MNIST (MNIST), we reduce \gls{flops} from 0.42,M to 0.12M (71\% reduction) and top-1 accuracy drops from 98.9\% to 96.0\% (–2.9\%).
We can see that the variations in pruning tolerances are consistent with the theories behind redundant neural network parameters. More complicated, and therefore deeper, networks such as VGG-16 contain substantial levels of redundancy regarding their parameters as discussed by \cite{Li2017}. For example, they have demonstrated that if we remove filters with the smallest L1-norm magnitudes, they can reduce the \gls{flops} for VGG-16 by 34\%, with less than 1\% accuracy loss on ImageNet with retraining. This is the result of parameters being over-parameterized: quite simply, the same feature can be learned by a number of filters which reduces the \gls{flops} of the model. Han et al.'s (2015) important "Deep Compression" paradigm \cite{Han2015} examined this issue, and they demonstrated that using magnitude pruning and retraining, they were able to remove over 92.5\% of the parameters in VGG-16 without any loss of accuracy. Smaller architectures like CCDF\_MNIST by nature of their size must since the architecture tends to operate near the minimal representational capacity needed to represent the task leave fewer parameters to prune. \cite{Blalock2020} conducted a meta-analysis study of 81 pruning studies and showed that compact models tend to exhibit larger relative accuracy losses at the same percentage of pruning, while large models exhibit larger improvement improvements due to their parameter slack.

Our schedule \[r_t = \min\bigl(0.2\,(1+0.1t),\,0.5\bigr)\]cautiously prune redundancy in earlier rounds, avoiding overshoot. This is similar to the Han et al. work, where iterative pruning and retraining eventually enables gradual learning to the more sparse configurations here. With the example of VGG-16, when pruning reduces redundancy aggressively and evenly on the first few rounds (20-30\%), it will remove very redundant filters, and even though the last few rounds were more measured and would have kept potentially meaningful features. The fact that the schedule worked on CCDF\_MNIST, which had minimal redundancy, also reinforces the idea that task-aware adaption is useful, even with redundancy. Even with small networks \cite{Catalan2025} will have bit-level redundancy, and if there is an understanding of this, this can either be exploited and/or learned more locally than in the large networks that pruning was applied to.

\section{Robustness under Radiation-Induced Faults}

The result of using Q\gls{seu} for the fault-injection experiments shows important relationships between compression and radiation robustness. In the case of CCDF\_MNIST, the faulty accuracy declines from 98.4\% to 78.0\% (–20.4\%) during three pruning steps, while for VGG-16, it decreases just 5.1\% (91.7\% to 86.6\%). 
It could be related to VGG's model structural redundancy. VGG-16's inherent architecture has multiple redundant computational pathways.i.e. each convolutional block has several parallel filters, and their outputs are summed or concatenated together - thus allowing the network to have the intact filters contribution backfill for the one that gets corrupted with a bit-flip - capturing the vital features.a prior work from \cite{Catalan2025}, noted that the sender's bit-level redundancy analysis showed VGG-16 weights could contain, 1-3 redundant bits per parameter, on average, which suggests - on average, when a bit is flipped - it will usually remain in the same quantization bin, having no impact on the activation. In contrast, lightweight models like MobileNet show up to 55\% "critical" bits as the value and the direct labeling of that bin changes due to a flip would yield larger perturbations.

The process of distribution of weights during training essentially enabled by the model's over-parameterization, VGG-16 does not distribute representational weight importance evenly, instead it is spread to many weights with the intention that no one number outcome has single point failure. 

The multiscale redundancy found both within convolutional blocks (filter outputs) and across depth of the network, allows transient faults to be hidden/corrected/gauged at multiple stages for inference without being dependent on expensive external error-correction. Thus, if adding structure redundancy to network architecture can be regarded as 'one' first line of defense against radiation induced single-event upsets, that could supplement plans for hardware ECC, remove redundant aspects from a systems holistic complexity. Hence, architectural redundancy of the VGG-16 model, makes it particularly amenable to allow the real-time, on-board deployment of AI in radiation-rich environments of space.

While the QSEU approach considers each bit-flipping as a \gls{seu}, it is known that actual radiation induced faults show clustering effects where there is one high-energy particle, that passes through multiple cells and causes them to flip on the way. The \gls{mbu} of flips that we show in figure x, is due to one particle strike, where the energy deposited along a track causes a single particle strike, which develops in parents - they caused local deposits of charge in nearby cells and caused switching - thus in effect, multiple flipping of bits.


However, in these experiments, and as executed in the \gls{fi} tool, multiple independent \gls{seu} are injected per inference rather than \gls{mbu}. Each bit-flip event is sampled uniformly random across the weight tensors with no spatial-temporal correlation between flips. This is meant to be the conservatively worst-case workload as one repeated single bit error occurring in the same inference time unit, without the hardware-level error-correction that may mask correlated adjacent flips. By decoupling the flips in the manner described, it makes a clear distinction in how the network inherently withstands faults discrete in nature. It also avoids mixing different spatial clustering effects in combination with structural redundancy of the model. Nevertheless, these results still apply to space borne implementations, simply because any real world \gls{mbu} will drive a faster degradation than is shown with independent \gls{seu}. From a practical point of view, especially in smaller architectures, many uncorrelated flips can result in critical impacts to feature extraction.

\section{Architecture Suitability and Design Recommendations}

For radiation-prone environments, our findings and recent literature suggest two key design principles:

\begin{itemize}
  \item \textbf{Moderate pruning of large models: } For applications with high reliability requirements (e.g., spacecraft navigation, remote sensing), a larger pre-trained model (e.g., VGG-16) fused down to 20-30\% of the original \gls{flops} provides a reasonable tradeoff between efficiency and fault tolerance. This is consistent with the suggestion of \cite{Catalan2025} that over-parameterization should be used for resiliency.
  
  \item \textbf{Conservative pruning of lightweight models:}For compact architectures like MobileNet or EfficientNet‑Lite, aggressive channel or filter pruning can severely diminish resilience to radiation‑induced bit errors. For example, we find that in our experiments, stripping away more than 50\% of the original \gls{flops} can result in a drop of the \gls{seu} tolerance on CIFAR‑10 by about 50 percent. In other words, we recommend a mild pruning regime, in which sparsity levels are only selected following fault-injection validation, so that efficiency gains do not compromise mission-critical robustness.

\end{itemize}

\section{Limitations and Future Directions}

While our framework demonstrates clear gains, several limitations remain:
The current QSEU simulations also assume random bit-flips are uniformly distributed and do not characterize the spatial/temporal correlation in real radiation events. Hardware-in-the-loop verification and testing can provide quantization noise and bit-flip spectra patterns, as well as device-specific or unique failure modes (similar to \cite{Goldstein2020}). The report by the AIAA in 2024 emphasizes the need for mission-specific fault profiles, because of polar or equatorial radiations experience different spectra of radiation.


\section{To what extent does model pruning impact the robustness of neural networks against \gls{seu}?}

By examining the empirical decrease under fault injection through the pruning strides, we can now examine the primary research question posed by this work: 

Our findings suggest that pruning impacts \gls{seu} resilience/robustness in a highly architecture-specific manner: it is strongly related to the amount of structural redundancy in the network. For a compact model such as \texttt{CCDF\_MNIST}, pruning results in a significant reduction in fault tolerance: for a 71\% reduction in FLOPS the accuracy under fault injection fell > 20 percentage points (from 98.4\% to 78.0\%). For \texttt{VGG-16}, a deeper and more over-parameterized model, similar pruning yielded a relatively modest 5.1\% reduction in faulty accuracy (from 91.7\% to 86.6\%). 

The implications of this observation indicate that while pruning is an effective approach to reduce computational costs, it also removes valuable redundancy that protects against bit level corruption, particularly with pruning approaches applied to architectures operating close to minimum representational capacity. Pruning must therefore be balanced--it is \texttt{acceptable to aggressively prune a large architecture that is redundant}, while even a modest amount of pruning on light-weight architectures may lead to catastrophic performance degradation under \gls{seu} conditions.